nfs:
  enabled: true
  # Use the output from the command below to set serverIP and serverName.
  # Inspect fileShares.0.name for the serverName and networks.0.ipAddresses.0
  # for the serverIP.
  #
  #   gcloud beta filestore instances describe nh-2020 --zone=us-east1-b
  #
  serverIP: 10.60.0.18
  serverName: nh

jupyterhub:
  debug:
    enabled: true

  ## ingress: should be enabled if we transition to use nginx-ingress +
  ## cert-manager.
  ##
  # ingress:
  #   enabled: true
  #   annotations:
  #     kubernetes.io/tls-acme: "true"
  #     kubernetes.io/ingress.class: nginx
  #   hosts:
  #     - hub.neurohackademy.org
  #   tls:
  #     - secretName: jupyterhub-tls
  #       hosts:
  #         - hub.neurohackademy.org

  prePuller:
    hook:
      enabled: false
    continuous:
      enabled: true

  scheduling:
    userScheduler:
      enabled: true
      replicas: 2
    podPriority:
      enabled: true
    userPlaceholder:
      enabled: true
      # These replicas will use the default configured CPU and memory requests,
      # which is overridden by c.KubeSpawner.profile_list choices by the user.
      # In our case, this means 24GB of memory or four time the size of a the
      # Small profile_list option that gives 6GB of memory. In other words, 50
      # replicas is like 200 Small users, which should be enough buffer to
      # manage a very large amount of simultaneously arriving users.
      replicas: 50
    corePods:
      nodeAffinity:
        matchNodePurpose: require
    userPods:
      nodeAffinity:
        matchNodePurpose: require

  singleuser:
    ## cmd: set this to start-singleuser.sh if we use a docker-stacks image,
    ## repo2docker does not come with that but the jupyterhub-singleuser command
    ## is part of JupyterHub though.
    ##
    # cmd: start-singleuser.sh
    defaultUrl: /lab
    startTimeout: 900
    ## cpu/memory requests:
    ## We want to fit as many users on a m1-ultramem-40 node but still ensure
    ## they get up to 24 GB of ram.
    cpu:
      guarantee: 0.36 # guarantee as much as possible for 110 pods (max per
                      # node because how k8s cluster was setup) to fit on a 40
                      # CPU machine
      limit: 16       # allow for a lot more CPU to be used
    memory:
      guarantee: 24G
    lifecycleHooks:
      postStart:
        exec:
          command:
            - "bash"
            - "/etc/singleuser/k8s-lifecycle-hook-post-start.sh"
    storage:
      storage:
        capacity: 10Gi
      ## extraVolumes is for the pod in general
      extraVolumes:
        - name: nh-nfs
          persistentVolumeClaim:
            claimName: nfs-pvc
        - name: nh-cache
          hostPath:
            path: /tmp/nh/data-cache
            type: Directory
        - name: user-etc-singleuser
          configMap:
            name: user-etc-singleuser
        - name: user-etc-profile-d
          configMap:
            name: user-etc-profile-d
        - name: user-usr-local-etc-jupyter
          configMap:
            name: user-usr-local-etc-jupyter
      ## extraVolumeMounts is for the pod's main container, not the initContainers
      extraVolumeMounts:
        - name: nh-nfs
          mountPath: /nh/curriculum
          subPath: curriculum
          readOnly: true
        - name: nh-cache
          mountPath: /nh/data
          subPath: data
          readOnly: true
        - mountPath: /etc/singleuser
          name: user-etc-singleuser
        - mountPath: /etc/profile.d/home-folder-replacements.sh
          name: user-etc-profile-d
          # NOTE: ConfigMap/Secret volumes using subPath doesn't automatically
          #       update after being mounted. This is fine though.
          subPath: home-folder-replacements.sh
        - mountPath: /usr/local/etc/jupyter
          name: user-usr-local-etc-jupyter
    ## initContainers:
    ## We may want this to ensure whatever dataset is mounted through NFS is
    ## readable for jovyan.
    initContainers:
      - name: chown-nfs-mount-to-jovyan
        image: busybox
        command:
          - "sh"
          - "-c"
          - "chown 1000:1000 /nh/data /nh/curriculum"
        securityContext:
          runAsUser: 0
        ## volumeMounts mounts the pod's volume defined through extraVolumes
        volumeMounts:
          - name: nh-nfs
            mountPath: /nh/data
            subPath: data
          - name: nh-nfs
            mountPath: /nh/curriculum
            subPath: curriculum

  hub:
    extraVolumes:
      - name: hub-etc-jupyterhub-acl
        secret:
          secretName: hub-etc-jupyterhub-acl
      - name: hub-etc-jupyterhub-templates
        configMap:
          name: hub-etc-jupyterhub-templates
      - name: hub-usr-local-share-jupyterhub-static-external
        configMap:
          name: hub-usr-local-share-jupyterhub-static-external
    extraVolumeMounts:
      - mountPath: /etc/jupyterhub/acl
        name: hub-etc-jupyterhub-acl
      - mountPath: /etc/jupyterhub/templates
        name: hub-etc-jupyterhub-templates
      - mountPath: /usr/local/share/jupyterhub/static/external
        name: hub-usr-local-share-jupyterhub-static-external
    extraConfig:
      # announcements: |
      #   c.JupyterHub.template_vars.update({
      #       'announcement': 'Any message we want to pass to instructors?',
      #   })
      performance: |
        # concurrentSpawnLimit
        # - documentation: https://jupyterhub.readthedocs.io/en/stable/api/app.html#jupyterhub.app.JupyterHub.concurrent_spawn_limit
        # - related discussion: https://github.com/jupyterhub/kubespawner/issues/419
        # - NOTE: 64 is the default value for z2jh, but for example this
        #         deployment has increased it to 200:
        #         https://github.com/2i2c-org/jupyterhub-utoronto/blob/staging/hub/values.yaml#L37
        c.JupyterHub.concurrent_spawn_limit = 200
      auth: |
        # Don't wait for users to press the orange button to login.
        c.Authenticator.auto_login = True
      templates: |
        # Help JupyterHub find the templates we may mount
        c.JupyterHub.template_paths.insert(0, "/etc/jupyterhub/templates")
      metrics: |
        # With this setting set to False, the /hub/metrics endpoint will be
        # publically accessible just like at hub.mybinder.org/hub/metrics is.
        c.JupyterHub.authenticate_prometheus = False
      workingDir: |
        # Override the working directory of /src/repo which repo2docker have set
        # to /home/jovyan instead, where we mount of files.
        c.KubeSpawner.extra_container_config = {
            "workingDir": "/home/jovyan",
        }
      options_form: |
        # Configure what spawn options users should see
        # ---------------------------------------------
        #
        # NOTE: setting c.KubeSpawner.profile_list directly is easier, but then
        #       we don't have the option to adjust it based on the individual
        #       user at a later point in time if we want.
        #
        # NOTE: c.KubeSpawner.options_form, defined in the Spawner base class,
        #       can be set to a fixed value, but it can also be a callable
        #       function that returns a value. If this returned value is falsy,
        #       no form will be rendered. In this case, we setup a callable
        #       function that relies on KubeSpawner's internal logic to create
        #       an options_form from the profile_list configuration.
        #
        #       ref: https://github.com/jupyterhub/jupyterhub/pull/2415
        #       ref: https://github.com/jupyterhub/jupyterhub/issues/2390
        #
        async def dynamic_options_form(self):
            self.profile_list = [
                {
                    'default': True,
                    'display_name': 'Small',
                    'description': '6GB RAM and up to 16 CPU cores.',
                    'kubespawner_override': { 'mem_guarantee':' 6G' },
                },
                {
                    'display_name': 'Medium',
                    'description': '12GB RAM and up to 16 CPU cores.',
                    'kubespawner_override': { 'mem_guarantee': '12G' },
                },
                {
                    'display_name': 'Large',
                    'description': '24GB RAM and up to 16 CPU cores.',
                    'kubespawner_override': { 'mem_guarantee': '24G' },
                },
            ]

            acl = read_acl()
            username = self.user.name
            if username in acl["instructors"] or username in acl["admins"]:
                self.profile_list.extend([
                    {
                        'display_name': 'Large - ~/data in read-only mode',
                        'description': 'As an instructor you otherwise have read/write access to the shared ~/data folder. This option can help you trial only having read access.',
                        'kubespawner_override': { 'mem_guarantee': '24G' },
                    }
                ])



            # NOTE: We let KubeSpawner inspect profile_list and decide what to
            #       return, it will return a falsy blank string if there is no
            #       profile_list, which makes no options form be presented.
            #
            # ref: https://github.com/jupyterhub/kubespawner/blob/37a80abb0a6c826e5c118a068fa1cf2725738038/kubespawner/spawner.py#L1885-L1935
            #
            return self._options_form_default()

        c.KubeSpawner.options_form = dynamic_options_form
      pre_spawn_hook: |
        # Configure storage details etc. based on the user and profile chosen
        # -------------------------------------------------------------------
        #
        async def pre_spawn_hook(spawner):
            username = spawner.user.name
            user_options = spawner.user_options # {'profile': 'display_name of chosen profile'}
            acl = read_acl()


            # Configure the pod's labels
            spawner.extra_labels.update({
                "hub.neurohackademy.org/is-admin": str(username in acl["admins"]).lower(),
                "hub.neurohackademy.org/is-instructor": str(username in acl["instructors"]).lower(),
                "hub.neurohackademy.org/is-participant": str(username in acl["participants"]).lower(),
                "hub.neurohackademy.org/profile": user_options.get("profile", "unknown").split(" ")[0].lower(),
            })

            # Configure the pod's storage
            read_only = not (username in acl["admins"] or username in acl["instructors"])
            read_only = read_only or "read" in user_options.get("profile")
            nh_volume_mount = {}
            if read_only:
                nh_volume_mount = {
                    "name": "nh-cache",       # nh-cache is a hostPath volume with cached NFS /data
                    "mountPath": "/nh/data",  # where it is made available in container
                    "readOnly": read_only,
                }
            else:
                nh_volume_mount = {
                    "name": "nh-nfs",         # nh is a NFS volume
                    "mountPath": "/nh/data",  # where it is made available in container
                    "subPath": "data",        # what in the PVC to mount (must be a relative path)
                    "readOnly": read_only,
                }
            # NOTE: If we would append spawner.volume_mounts we would risk
            # getting multiple entries in this in a future spawn. Due to that,
            # let's first remove mounts like the one we are about to add, then
            # add it.
            volume_mounts = [m for m in spawner.volume_mounts if m["mountPath"] != nh_volume_mount["mountPath"]]
            volume_mounts.append(nh_volume_mount)
            spawner.volume_mounts = volume_mounts

            # Configure the pod's container's environment variables
            spawner.environment.update({})

        c.KubeSpawner.pre_spawn_hook = pre_spawn_hook

  proxy:
    https:
      enabled: true
      hosts: [hub.neurohackademy.org]
    service:
      type: LoadBalancer
      loadBalancerIP: 34.75.11.207

  cull:
    enabled: true
    timeout: 7200 # 2 hours in seconds
    maxAge: 0 # Allow pods to run forever

# Reference on the Grafana Helm chart's configuration options:
# https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
grafana:
  # Reference on Grafana's configuration options:
  # http://docs.grafana.org/installation/configuration
  grafana.ini:
    log:
      level: debug
    server:
      domain: hub.neurohackademy.org
      # NOTE: Don't use %(protocol)s in root_url, but hardcode https. If not, it
      #       will when redirecting the user to external authentication set with
      #       a redirect back query parameter to use http instead of https,
      #       which will be wrong. This is because the TLS termination is done
      #       without Grafanas knowledge by the ingress controller. If we would
      #       specify protocol to be https, then it would want to do the TLS
      #       termination itself so that also would fail.
      root_url: 'https://%(domain)s/services/grafana'
      serve_from_sub_path: true
      enforce_domain: true
      enable_gzip: true
      router_logging: true
